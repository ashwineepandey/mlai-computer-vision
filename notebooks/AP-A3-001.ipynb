{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import utils as ut\n",
    "import log\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ut.timer\n",
    "def load_data():\n",
    "    # Load the Fashion-MNIST dataset\n",
    "    (X_train, _), (_, _) = fashion_mnist.load_data()\n",
    "    X_train = X_train / 127.5 - 1.0 # Normalize the images to [-1, 1]\n",
    "    return np.expand_dims(X_train, axis=3)\n",
    "\n",
    "\n",
    "# Generator\n",
    "@ut.timer\n",
    "def create_generator():\n",
    "    generator = Sequential([\n",
    "        Dense(128, input_dim=100),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(1024),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(28 * 28 * 1, activation='tanh'),\n",
    "        Reshape((28, 28, 1))\n",
    "    ])\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "@ut.timer\n",
    "def create_discriminator():\n",
    "    discriminator = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Flatten(),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "# GAN\n",
    "@ut.timer\n",
    "def create_gan(discriminator, generator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(100,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    return gan\n",
    "\n",
    "\n",
    "@ut.timer\n",
    "def compile_models(discriminator, generator):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "    gan = create_gan(discriminator, generator)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    return gan\n",
    "\n",
    "\n",
    "@ut.timer\n",
    "def sample_images(generator, epoch, img_out_path, datetime):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images from [-1, 1] to [0, 1]\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(f\"{img_out_path}/{datetime}_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Train GAN\n",
    "@ut.timer\n",
    "def train_gan(X_train, generator, discriminator, gan, epochs, batch_size, sample_interval, img_out_path, datetime):\n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_imgs = X_train[idx]\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        \n",
    "        real_y = np.ones((batch_size, 1))\n",
    "        fake_y = np.zeros((batch_size, 1))\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, real_y)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake_y)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        real_y = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, real_y)\n",
    "        \n",
    "        if epoch % sample_interval == 0:\n",
    "            logger.info(f\"Epoch {epoch}, D-Loss: {d_loss[0]}, G-Loss: {g_loss}\")\n",
    "            sample_images(generator, epoch, img_out_path, datetime)\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "\n",
    "def save_models(generator, discriminator, gan, model_out_path, datetime):\n",
    "    generator.save(f\"{model_out_path}{datetime}_generator.h5\")\n",
    "    discriminator.save(f\"{model_out_path}{datetime}_discriminator.h5\")\n",
    "    gan.save(f\"{model_out_path}{datetime}_gan.h5\")\n",
    "    logger.info(\"Models saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06-May-23 14:36:55 - INFO - Starting 'load_config'.\n",
      "06-May-23 14:36:56 - INFO - Finished 'load_config' in 0.0515 secs.\n",
      "06-May-23 14:36:56 - INFO - Starting 'load_data'.\n",
      "06-May-23 14:36:56 - INFO - Finished 'load_data' in 0.4083 secs.\n",
      "06-May-23 14:36:56 - INFO - Starting 'create_generator'.\n",
      "06-May-23 14:36:56 - INFO - Finished 'create_generator' in 0.0397 secs.\n",
      "06-May-23 14:36:56 - INFO - Starting 'create_discriminator'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06-May-23 14:36:56 - INFO - Finished 'create_discriminator' in 0.0141 secs.\n",
      "06-May-23 14:36:56 - INFO - Starting 'compile_models'.\n",
      "06-May-23 14:36:56 - INFO - Starting 'create_gan'.\n",
      "06-May-23 14:36:56 - INFO - Finished 'create_gan' in 0.0232 secs.\n",
      "06-May-23 14:36:56 - INFO - Finished 'compile_models' in 0.0295 secs.\n",
      "06-May-23 14:36:56 - INFO - Starting 'train_gan'.\n",
      "2023-05-06 14:36:56.547798: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-06 14:36:56.692833: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x2d129bb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x2d674f4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 14:36:57.041730: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "06-May-23 14:36:57 - INFO - Epoch 0, D-Loss: 0.7250036001205444, G-Loss: 0.657907247543335\n",
      "06-May-23 14:36:57 - INFO - Starting 'sample_images'.\n",
      "2023-05-06 14:36:57.261960: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "06-May-23 14:36:57 - INFO - Finished 'sample_images' in 0.3681 secs.\n",
      "06-May-23 14:36:57 - INFO - Epoch 4, D-Loss: 0.6821849551051855, G-Loss: 0.16276264190673828\n",
      "06-May-23 14:36:57 - INFO - Starting 'sample_images'.\n",
      "06-May-23 14:36:57 - INFO - Finished 'sample_images' in 0.1584 secs.\n",
      "06-May-23 14:36:58 - INFO - Epoch 8, D-Loss: 0.6865379419177771, G-Loss: 0.41157853603363037\n",
      "06-May-23 14:36:58 - INFO - Starting 'sample_images'.\n",
      "06-May-23 14:36:58 - INFO - Finished 'sample_images' in 0.2436 secs.\n",
      "06-May-23 14:36:58 - INFO - Epoch 12, D-Loss: 0.5012757387012243, G-Loss: 0.9639813899993896\n",
      "06-May-23 14:36:58 - INFO - Starting 'sample_images'.\n",
      "06-May-23 14:36:58 - INFO - Finished 'sample_images' in 0.1589 secs.\n",
      "06-May-23 14:36:59 - INFO - Epoch 16, D-Loss: 0.3451346941292286, G-Loss: 1.5283849239349365\n",
      "06-May-23 14:36:59 - INFO - Starting 'sample_images'.\n",
      "06-May-23 14:36:59 - INFO - Finished 'sample_images' in 0.1847 secs.\n",
      "06-May-23 14:36:59 - INFO - Finished 'train_gan' in 2.9067 secs.\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "conf = ut.load_config()\n",
    "X_train = load_data()\n",
    "generator = create_generator()\n",
    "discriminator = create_discriminator()\n",
    "gan = compile_models(discriminator, generator)\n",
    "dt = ut.get_datetime()\n",
    "# Set parameters and train the GAN\n",
    "generator, discriminator, gan = train_gan(X_train, generator, discriminator, gan, conf.a3.gan_params.epochs, conf.a3.gan_params.batch_size, conf.a3.gan_params.sample_interval, conf.a3.paths.training_inspection_plots, dt)\n",
    "# Save models\n",
    "save_models(generator, discriminator, gan, conf.a3.paths.model, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_cvgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
